
Great tweak—keeping it simple with plain Python lists. I’ll keep explanations short for your slides and give you comment-free code blocks you can drop into a training notebook.

Means (arithmetic, geometric, harmonic)

When to use

Arithmetic: normal average for additive quantities (e.g., monthly income).

Geometric: compounding/multiplicative effects (returns, growth).

Harmonic: averaging rates/ratios; emphasizes smaller values.


import math

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

arith_mean = sum(incomes) / len(incomes)
geom_mean = math.prod(incomes) ** (1/len(incomes))
harm_mean = len(incomes) / sum(1/x for x in incomes)

print(arith_mean, geom_mean, harm_mean)

Median (odd vs even)

Why: robust to outliers/skew.

import statistics as stats

income_odd = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500]
income_even = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000]

median_odd = stats.median(income_odd)
median_even = stats.median(income_even)

print(median_odd, median_even)

Mode on loan terms (months)

Definitions
Unimodal: one most frequent term.
Bimodal: two tied most frequent terms.
Multimodal: three or more tied most frequent terms.

import statistics as stats

loan_terms_uni = [12,12,12,6,24,12,18,12,24,36]
loan_terms_bi = [12,24,12,24,6,18,36,12,24,18,36,6]
loan_terms_multi = [6,12,18,6,12,18,6,12,18]

print(stats.multimode(loan_terms_uni))
print(stats.multimode(loan_terms_bi))
print(stats.multimode(loan_terms_multi))

Imputing missing loan terms when there are multiple modes

Choices

Random among tied modes.

Proportional to observed frequencies.

Conditional within a subgroup (best for ML when a predictor exists).


import random
import statistics as stats
from collections import Counter

random.seed(7)
loan_terms = [12,6,18,12,18,6,None,None,None]
observed = [x for x in loan_terms if x is not None]
modes = stats.multimode(observed)
filled_random = [x if x is not None else random.choice(modes) for x in loan_terms]
print(filled_random)

import random
from collections import Counter

random.seed(7)
loan_terms = [12,6,18,12,18,6,None,None,None]
obs_counts = Counter(x for x in loan_terms if x is not None)
cats = list(obs_counts.keys())
weights = [obs_counts[c]/sum(obs_counts.values()) for c in cats]
filled_prop = [x if x is not None else random.choices(cats, weights=weights, k=1)[0] for x in loan_terms]
print(filled_prop)

import random
import statistics as stats

random.seed(7)
segments = ["Retail","Retail","SME","SME","Retail","SME","Retail","SME","Retail"]
loan_terms = [12,6,18,12,18,6,None,None,None]

filled_conditional = loan_terms[:]
for seg in set(segments):
    idx = [i for i,(s,v) in enumerate(zip(segments,loan_terms)) if s==seg]
    obs = [loan_terms[i] for i in idx if loan_terms[i] is not None]
    if not obs:
        continue
    modes = stats.multimode(obs)
    choice = random.choice(modes)
    for i in idx:
        if filled_conditional[i] is None:
            filled_conditional[i] = choice

print(list(zip(segments, filled_conditional)))

Want quick mini-exercises to pair with this (e.g., “change one term to 60 and predict which summary shifts most”)?



Alright — let’s make this training-friendly with plain lists, banking-themed, and short examples you can reuse in slides.


---

Geometric Mean Example

Scenario:
An investment grows by 10% the first year, 20% the second year, and -15% the third year.
The geometric mean tells you the average yearly growth rate, accounting for compounding.

import math

growth_factors = [1.10, 1.20, 0.85]  
geom_mean = math.prod(growth_factors) ** (1/len(growth_factors)) - 1  
print(f"Average annual growth rate: {geom_mean*100:.2f}%")

Why geometric mean here?
Because returns multiply over time — you can’t just average percentages directly.


---

Harmonic Mean Example

Scenario:
A bank measures the average interest rate across three loans of equal principal:

Loan 1: 4%

Loan 2: 6%

Loan 3: 8%


interest_rates = [0.04, 0.06, 0.08]  
harm_mean = len(interest_rates) / sum(1/r for r in interest_rates)  
print(f"Average interest rate: {harm_mean*100:.2f}%")

Why harmonic mean here?
It’s best for rates (like speed, interest, P/E ratios) where you want each observation to have equal weight, not just the average of the rates.


---

If you want, I can also give you one visual slide-ready diagram showing when to use arithmetic, geometric, and harmonic mean so learners instantly remember. That would make it pop in your GenuDeck. Do you want me to prepare that?




Got it — I’ll make it as simple as possible, still using your banking-style dummy data, plain Python lists, and no comments.


---

Absolute Measures

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
median_val = stats.median(incomes)

range_val = max(incomes) - min(incomes)
mean_dev = sum(abs(x - mean_val) for x in incomes) / len(incomes)
variance_val = stats.pvariance(incomes)
sd_val = stats.pstdev(incomes)

print(range_val)
print(mean_dev)
print(variance_val)
print(sd_val)


---

Relative Measures

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
mean_dev = sum(abs(x - mean_val) for x in incomes) / len(incomes)
sd_val = stats.pstdev(incomes)

coef_mean_dev = mean_dev / mean_val
coef_var = sd_val / mean_val

print(coef_mean_dev)
print(coef_var)


---

Do you want me to merge both parts into a single short code block so it runs in one go for your training session? That way your learners won’t have to run two scripts.



Got it—plain lists, no data generation, and code without comments. I’ll cover the absolute (Range, Mean Deviation, Variance, SD) and relative (Coefficient of Mean Deviation, Coefficient of Variation) measures with banking-style dummy data.

Absolute measures (one dataset)

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
median_val = stats.median(incomes)

range_val = max(incomes) - min(incomes)

mean_deviation_mean = sum(abs(x - mean_val) for x in incomes) / len(incomes)
mean_deviation_median = sum(abs(x - median_val) for x in incomes) / len(incomes)

sample_variance = stats.variance(incomes)
population_variance = stats.pvariance(incomes)

sample_sd = stats.stdev(incomes)
population_sd = stats.pstdev(incomes)

print(range_val)
print(mean_deviation_mean, mean_deviation_median)
print(sample_variance, population_variance)
print(sample_sd, population_sd)

Relative measures (same dataset)

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
mean_deviation_mean = sum(abs(x - mean_val) for x in incomes) / len(incomes)
sample_sd = stats.stdev(incomes)

coef_mean_deviation = mean_deviation_mean / mean_val
coef_variation = sample_sd / mean_val

print(coef_mean_deviation)
print(coef_variation)

Optional comparison of CV on two groups

import statistics as stats

branch_A_incomes = [42000, 48000, 50000, 52000, 56000, 60000]
branch_B_incomes = [28000, 36000, 44000, 52000, 60000, 68000]

cv_A = stats.stdev(branch_A_incomes) / (sum(branch_A_incomes)/len(branch_A_incomes))
cv_B = stats.stdev(branch_B_incomes) / (sum(branch_B_incomes)/len(branch_B_incomes))

print(cv_A, cv_B)

Want me to align the formulas exactly with how you define them on the slide (e.g., sample vs population) before you present?



Got it — I’ll make you an interactive Python code for skewness & kurtosis where you can change the shape, spread, and center of the data and see how the metrics change.
This will work best in Jupyter Notebook using ipywidgets so you can “play” with sliders.


---

# Interactive Skewness & Kurtosis Demo
# ------------------------------------
# This code lets you tweak data parameters and watch skewness/kurtosis update live.

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis
import ipywidgets as widgets
from ipywidgets import interact

# Function to generate and analyze data
def skew_kurt_demo(dist="normal", n=1000, mean=0, std=1, skew_factor=0, heavy_tail=0):
    """
    dist: 'normal', 'uniform', 'exponential'
    n: number of samples
    mean, std: for normal/uniform adjustments
    skew_factor: adds skew via exponential transformation
    heavy_tail: adds kurtosis via t-distribution-like transformation
    """
    # Base distribution
    if dist == "normal":
        data = np.random.normal(mean, std, n)
    elif dist == "uniform":
        data = np.random.uniform(mean - std, mean + std, n)
    elif dist == "exponential":
        data = np.random.exponential(std, n) + mean
    else:
        raise ValueError("Choose: normal, uniform, exponential")
    
    # Apply skew factor (positive = right skew, negative = left skew)
    if skew_factor != 0:
        data = data ** (1 + skew_factor)

    # Apply heavy tails (positive = heavier tails)
    if heavy_tail > 0:
        tail_noise = np.random.standard_t(df=max(1, 10 - heavy_tail*9), size=n)
        data = mean + std * tail_noise

    # Calculate skewness & kurtosis
    data_skew = skew(data)
    data_kurt = kurtosis(data)  # excess kurtosis (Normal = 0)
    
    # Plot histogram
    plt.figure(figsize=(6, 4))
    plt.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black')
    plt.title(f"Distribution: {dist.capitalize()} | Skew={data_skew:.2f} | Kurtosis={data_kurt:.2f}")
    plt.xlabel("Value")
    plt.ylabel("Density")
    plt.axvline(np.mean(data), color='red', linestyle='--', label=f"Mean={np.mean(data):.2f}")
    plt.legend()
    plt.show()

# Interactive controls
interact(
    skew_kurt_demo,
    dist=widgets.Dropdown(options=['normal', 'uniform', 'exponential'], value='normal', description='Distribution'),
    n=widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples'),
    mean=widgets.FloatSlider(value=0, min=-5, max=5, step=0.5, description='Mean'),
    std=widgets.FloatSlider(value=1, min=0.5, max=5, step=0.5, description='Std Dev'),
    skew_factor=widgets.FloatSlider(value=0, min=-1, max=1, step=0.1, description='Skew Factor'),
    heavy_tail=widgets.FloatSlider(value=0, min=0, max=1, step=0.1, description='Heavy Tail')
)


---

How this works

skew_factor

Positive → right skew

Negative → left skew


heavy_tail

0 → normal tails

Higher → heavier tails (higher kurtosis)


You can switch between:

Normal (baseline Gaussian)

Uniform (flat, low kurtosis)

Exponential (naturally right-skewed)


Output shows:

Skewness: measure of symmetry

Kurtosis: “tailedness” (Excess kurtosis: normal = 0)




---

If you want, I can also add a live curve overlay of a normal distribution for comparison so that people can see how skewness and kurtosis distort the shape. That would make it even better for training.

Do you want me to add that visual comparison?



Here’s the Python code to create a box plot for the same column (LoanAmount) shown in your histogram:

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.boxplot(df['LoanAmount'].dropna(), vert=False)
plt.title('Before Outlier Treatment')
plt.xlabel('Loan Amount')
plt.show()

Notes:

dropna() ensures missing values are ignored.

vert=False makes the box plot horizontal like your histogram for easier comparison.

You can replace df['LoanAmount'] with your actual DataFrame and column name.


Do you want me to also




Here’s clean, ready-to-run Python code for all three outlier removal approaches — IQR method, Z-score method, and Capping method — for your LoanAmount column.


---

1. IQR Method

import pandas as pd
import numpy as np

# IQR method
Q1 = df['LoanAmount'].quantile(0.25)
Q3 = df['LoanAmount'].quantile(0.75)
IQR = Q3 - Q1

# Keep only values within the IQR bounds
df_iqr = df[(df['LoanAmount'] >= Q1 - 1.5 * IQR) & (df['LoanAmount'] <= Q3 + 1.5 * IQR)]


---

2. Z-Score Method

from scipy import stats

# Calculate Z-scores
z_scores = np.abs(stats.zscore(df['LoanAmount'].dropna()))
df_zscore = df[z_scores < 3]  # Keep only rows where z < 3


---

3. Capping Method (Winsorization)

# Define caps
lower_cap = df['LoanAmount'].quantile(0.05)  # 5th percentile
upper_cap = df['LoanAmount'].quantile(0.95)  # 95th percentile

# Apply capping
df_cap = df.copy()
df_cap['LoanAmount'] = np.where(df_cap['LoanAmount'] < lower_cap, lower_cap,
                                np.where(df_cap['LoanAmount'] > upper_cap, upper_cap, df_cap['LoanAmount']))


---

✅ Summary:

IQR: Removes rows outside the IQR range.

Z-score: Removes rows with extreme standard deviations.

Capping: Replaces outliers with nearest percentile values instead of removing them.



---

If you want, I can also give you a side-by-side box plot before and after for all three methods so you can visually see the difference. That would make it much easier to compare.





Your error comes from two issues: you replaced the DataFrame with the z-score array, and you’re using a column name that doesn’t exist in that DF. Use a separate variable for the z-scores and the actual column name (in your screenshot it’s Income, not LoanAmount).

Here’s the correct, minimal z-score outlier filter:

from scipy.stats import zscore

df_z = loan_df.copy()
col = 'Income'  # change to 'LoanAmount' if that's the real column

z = zscore(df_z[col].astype(float), nan_policy='omit')
df_z = df_z.loc[(abs(z) < 3) | df_z[col].isna()]

df_z.head()

(If you want the plot after filtering:)

ax = df_z[col].hist(bins=20, figsize=(12,4))



if p_value < alpha:
    print(f"Decision: Reject the null hypothesis (p = {p_value:.4f} < α = {alpha})\n"
          "Inference: There is statistically significant evidence that the coin is biased.")
else:
    print(f"Decision: Fail to reject the null hypothesis (p = {p_value:.4f} ≥ α = {alpha})\n"
          "Inference: There is not enough statistical evidence to say the coin is biased.")



Here’s a clean way to phrase your interpretation for the GPT prompt, based on the image and your explanation:


---

Interpretation:
When tossing a fair coin, the probability of getting a tail on any single toss is 0.5. However, as we observe increasingly unlikely sequences — such as getting two tails in a row, then three tails, and eventually five tails — the probability of those events occurring purely by chance becomes much smaller. In statistical terms, the p-value decreases as the observed outcome becomes more extreme, providing stronger evidence against the null hypothesis that the coin is fair.


---

If you want, I can also make a more visual step-by-step explanation linking the sequence of tails to decreasing p-values so it matches exactly with the image’s coin graphics. Would you like me to prepare that?




