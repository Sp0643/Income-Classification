
Great tweak—keeping it simple with plain Python lists. I’ll keep explanations short for your slides and give you comment-free code blocks you can drop into a training notebook.

Means (arithmetic, geometric, harmonic)

When to use

Arithmetic: normal average for additive quantities (e.g., monthly income).

Geometric: compounding/multiplicative effects (returns, growth).

Harmonic: averaging rates/ratios; emphasizes smaller values.


import math

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

arith_mean = sum(incomes) / len(incomes)
geom_mean = math.prod(incomes) ** (1/len(incomes))
harm_mean = len(incomes) / sum(1/x for x in incomes)

print(arith_mean, geom_mean, harm_mean)

Median (odd vs even)

Why: robust to outliers/skew.

import statistics as stats

income_odd = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500]
income_even = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000]

median_odd = stats.median(income_odd)
median_even = stats.median(income_even)

print(median_odd, median_even)

Mode on loan terms (months)

Definitions
Unimodal: one most frequent term.
Bimodal: two tied most frequent terms.
Multimodal: three or more tied most frequent terms.

import statistics as stats

loan_terms_uni = [12,12,12,6,24,12,18,12,24,36]
loan_terms_bi = [12,24,12,24,6,18,36,12,24,18,36,6]
loan_terms_multi = [6,12,18,6,12,18,6,12,18]

print(stats.multimode(loan_terms_uni))
print(stats.multimode(loan_terms_bi))
print(stats.multimode(loan_terms_multi))

Imputing missing loan terms when there are multiple modes

Choices

Random among tied modes.

Proportional to observed frequencies.

Conditional within a subgroup (best for ML when a predictor exists).


import random
import statistics as stats
from collections import Counter

random.seed(7)
loan_terms = [12,6,18,12,18,6,None,None,None]
observed = [x for x in loan_terms if x is not None]
modes = stats.multimode(observed)
filled_random = [x if x is not None else random.choice(modes) for x in loan_terms]
print(filled_random)

import random
from collections import Counter

random.seed(7)
loan_terms = [12,6,18,12,18,6,None,None,None]
obs_counts = Counter(x for x in loan_terms if x is not None)
cats = list(obs_counts.keys())
weights = [obs_counts[c]/sum(obs_counts.values()) for c in cats]
filled_prop = [x if x is not None else random.choices(cats, weights=weights, k=1)[0] for x in loan_terms]
print(filled_prop)

import random
import statistics as stats

random.seed(7)
segments = ["Retail","Retail","SME","SME","Retail","SME","Retail","SME","Retail"]
loan_terms = [12,6,18,12,18,6,None,None,None]

filled_conditional = loan_terms[:]
for seg in set(segments):
    idx = [i for i,(s,v) in enumerate(zip(segments,loan_terms)) if s==seg]
    obs = [loan_terms[i] for i in idx if loan_terms[i] is not None]
    if not obs:
        continue
    modes = stats.multimode(obs)
    choice = random.choice(modes)
    for i in idx:
        if filled_conditional[i] is None:
            filled_conditional[i] = choice

print(list(zip(segments, filled_conditional)))

Want quick mini-exercises to pair with this (e.g., “change one term to 60 and predict which summary shifts most”)?



Alright — let’s make this training-friendly with plain lists, banking-themed, and short examples you can reuse in slides.


---

Geometric Mean Example

Scenario:
An investment grows by 10% the first year, 20% the second year, and -15% the third year.
The geometric mean tells you the average yearly growth rate, accounting for compounding.

import math

growth_factors = [1.10, 1.20, 0.85]  
geom_mean = math.prod(growth_factors) ** (1/len(growth_factors)) - 1  
print(f"Average annual growth rate: {geom_mean*100:.2f}%")

Why geometric mean here?
Because returns multiply over time — you can’t just average percentages directly.


---

Harmonic Mean Example

Scenario:
A bank measures the average interest rate across three loans of equal principal:

Loan 1: 4%

Loan 2: 6%

Loan 3: 8%


interest_rates = [0.04, 0.06, 0.08]  
harm_mean = len(interest_rates) / sum(1/r for r in interest_rates)  
print(f"Average interest rate: {harm_mean*100:.2f}%")

Why harmonic mean here?
It’s best for rates (like speed, interest, P/E ratios) where you want each observation to have equal weight, not just the average of the rates.


---

If you want, I can also give you one visual slide-ready diagram showing when to use arithmetic, geometric, and harmonic mean so learners instantly remember. That would make it pop in your GenuDeck. Do you want me to prepare that?




Got it — I’ll make it as simple as possible, still using your banking-style dummy data, plain Python lists, and no comments.


---

Absolute Measures

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
median_val = stats.median(incomes)

range_val = max(incomes) - min(incomes)
mean_dev = sum(abs(x - mean_val) for x in incomes) / len(incomes)
variance_val = stats.pvariance(incomes)
sd_val = stats.pstdev(incomes)

print(range_val)
print(mean_dev)
print(variance_val)
print(sd_val)


---

Relative Measures

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
mean_dev = sum(abs(x - mean_val) for x in incomes) / len(incomes)
sd_val = stats.pstdev(incomes)

coef_mean_dev = mean_dev / mean_val
coef_var = sd_val / mean_val

print(coef_mean_dev)
print(coef_var)


---

Do you want me to merge both parts into a single short code block so it runs in one go for your training session? That way your learners won’t have to run two scripts.



Got it—plain lists, no data generation, and code without comments. I’ll cover the absolute (Range, Mean Deviation, Variance, SD) and relative (Coefficient of Mean Deviation, Coefficient of Variation) measures with banking-style dummy data.

Absolute measures (one dataset)

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
median_val = stats.median(incomes)

range_val = max(incomes) - min(incomes)

mean_deviation_mean = sum(abs(x - mean_val) for x in incomes) / len(incomes)
mean_deviation_median = sum(abs(x - median_val) for x in incomes) / len(incomes)

sample_variance = stats.variance(incomes)
population_variance = stats.pvariance(incomes)

sample_sd = stats.stdev(incomes)
population_sd = stats.pstdev(incomes)

print(range_val)
print(mean_deviation_mean, mean_deviation_median)
print(sample_variance, population_variance)
print(sample_sd, population_sd)

Relative measures (same dataset)

import statistics as stats

incomes = [42000, 51000, 49500, 55000, 60000, 46000, 52500, 57000, 58500, 310000]

mean_val = sum(incomes) / len(incomes)
mean_deviation_mean = sum(abs(x - mean_val) for x in incomes) / len(incomes)
sample_sd = stats.stdev(incomes)

coef_mean_deviation = mean_deviation_mean / mean_val
coef_variation = sample_sd / mean_val

print(coef_mean_deviation)
print(coef_variation)

Optional comparison of CV on two groups

import statistics as stats

branch_A_incomes = [42000, 48000, 50000, 52000, 56000, 60000]
branch_B_incomes = [28000, 36000, 44000, 52000, 60000, 68000]

cv_A = stats.stdev(branch_A_incomes) / (sum(branch_A_incomes)/len(branch_A_incomes))
cv_B = stats.stdev(branch_B_incomes) / (sum(branch_B_incomes)/len(branch_B_incomes))

print(cv_A, cv_B)

Want me to align the formulas exactly with how you define them on the slide (e.g., sample vs population) before you present?



Got it — I’ll make you an interactive Python code for skewness & kurtosis where you can change the shape, spread, and center of the data and see how the metrics change.
This will work best in Jupyter Notebook using ipywidgets so you can “play” with sliders.


---

# Interactive Skewness & Kurtosis Demo
# ------------------------------------
# This code lets you tweak data parameters and watch skewness/kurtosis update live.

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis
import ipywidgets as widgets
from ipywidgets import interact

# Function to generate and analyze data
def skew_kurt_demo(dist="normal", n=1000, mean=0, std=1, skew_factor=0, heavy_tail=0):
    """
    dist: 'normal', 'uniform', 'exponential'
    n: number of samples
    mean, std: for normal/uniform adjustments
    skew_factor: adds skew via exponential transformation
    heavy_tail: adds kurtosis via t-distribution-like transformation
    """
    # Base distribution
    if dist == "normal":
        data = np.random.normal(mean, std, n)
    elif dist == "uniform":
        data = np.random.uniform(mean - std, mean + std, n)
    elif dist == "exponential":
        data = np.random.exponential(std, n) + mean
    else:
        raise ValueError("Choose: normal, uniform, exponential")
    
    # Apply skew factor (positive = right skew, negative = left skew)
    if skew_factor != 0:
        data = data ** (1 + skew_factor)

    # Apply heavy tails (positive = heavier tails)
    if heavy_tail > 0:
        tail_noise = np.random.standard_t(df=max(1, 10 - heavy_tail*9), size=n)
        data = mean + std * tail_noise

    # Calculate skewness & kurtosis
    data_skew = skew(data)
    data_kurt = kurtosis(data)  # excess kurtosis (Normal = 0)
    
    # Plot histogram
    plt.figure(figsize=(6, 4))
    plt.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black')
    plt.title(f"Distribution: {dist.capitalize()} | Skew={data_skew:.2f} | Kurtosis={data_kurt:.2f}")
    plt.xlabel("Value")
    plt.ylabel("Density")
    plt.axvline(np.mean(data), color='red', linestyle='--', label=f"Mean={np.mean(data):.2f}")
    plt.legend()
    plt.show()

# Interactive controls
interact(
    skew_kurt_demo,
    dist=widgets.Dropdown(options=['normal', 'uniform', 'exponential'], value='normal', description='Distribution'),
    n=widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples'),
    mean=widgets.FloatSlider(value=0, min=-5, max=5, step=0.5, description='Mean'),
    std=widgets.FloatSlider(value=1, min=0.5, max=5, step=0.5, description='Std Dev'),
    skew_factor=widgets.FloatSlider(value=0, min=-1, max=1, step=0.1, description='Skew Factor'),
    heavy_tail=widgets.FloatSlider(value=0, min=0, max=1, step=0.1, description='Heavy Tail')
)


---

How this works

skew_factor

Positive → right skew

Negative → left skew


heavy_tail

0 → normal tails

Higher → heavier tails (higher kurtosis)


You can switch between:

Normal (baseline Gaussian)

Uniform (flat, low kurtosis)

Exponential (naturally right-skewed)


Output shows:

Skewness: measure of symmetry

Kurtosis: “tailedness” (Excess kurtosis: normal = 0)




---

If you want, I can also add a live curve overlay of a normal distribution for comparison so that people can see how skewness and kurtosis distort the shape. That would make it even better for training.

Do you want me to add that visual comparison?



Here’s the Python code to create a box plot for the same column (LoanAmount) shown in your histogram:

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.boxplot(df['LoanAmount'].dropna(), vert=False)
plt.title('Before Outlier Treatment')
plt.xlabel('Loan Amount')
plt.show()

Notes:

dropna() ensures missing values are ignored.

vert=False makes the box plot horizontal like your histogram for easier comparison.

You can replace df['LoanAmount'] with your actual DataFrame and column name.


Do you want me to also




Here’s clean, ready-to-run Python code for all three outlier removal approaches — IQR method, Z-score method, and Capping method — for your LoanAmount column.


---

1. IQR Method

import pandas as pd
import numpy as np

# IQR method
Q1 = df['LoanAmount'].quantile(0.25)
Q3 = df['LoanAmount'].quantile(0.75)
IQR = Q3 - Q1

# Keep only values within the IQR bounds
df_iqr = df[(df['LoanAmount'] >= Q1 - 1.5 * IQR) & (df['LoanAmount'] <= Q3 + 1.5 * IQR)]


---

2. Z-Score Method

from scipy import stats

# Calculate Z-scores
z_scores = np.abs(stats.zscore(df['LoanAmount'].dropna()))
df_zscore = df[z_scores < 3]  # Keep only rows where z < 3


---

3. Capping Method (Winsorization)

# Define caps
lower_cap = df['LoanAmount'].quantile(0.05)  # 5th percentile
upper_cap = df['LoanAmount'].quantile(0.95)  # 95th percentile

# Apply capping
df_cap = df.copy()
df_cap['LoanAmount'] = np.where(df_cap['LoanAmount'] < lower_cap, lower_cap,
                                np.where(df_cap['LoanAmount'] > upper_cap, upper_cap, df_cap['LoanAmount']))


---

✅ Summary:

IQR: Removes rows outside the IQR range.

Z-score: Removes rows with extreme standard deviations.

Capping: Replaces outliers with nearest percentile values instead of removing them.



---

If you want, I can also give you a side-by-side box plot before and after for all three methods so you can visually see the difference. That would make it much easier to compare.





Your error comes from two issues: you replaced the DataFrame with the z-score array, and you’re using a column name that doesn’t exist in that DF. Use a separate variable for the z-scores and the actual column name (in your screenshot it’s Income, not LoanAmount).

Here’s the correct, minimal z-score outlier filter:

from scipy.stats import zscore

df_z = loan_df.copy()
col = 'Income'  # change to 'LoanAmount' if that's the real column

z = zscore(df_z[col].astype(float), nan_policy='omit')
df_z = df_z.loc[(abs(z) < 3) | df_z[col].isna()]

df_z.head()

(If you want the plot after filtering:)

ax = df_z[col].hist(bins=20, figsize=(12,4))



if p_value < alpha:
    print(f"Decision: Reject the null hypothesis (p = {p_value:.4f} < α = {alpha})\n"
          "Inference: There is statistically significant evidence that the coin is biased.")
else:
    print(f"Decision: Fail to reject the null hypothesis (p = {p_value:.4f} ≥ α = {alpha})\n"
          "Inference: There is not enough statistical evidence to say the coin is biased.")



Here’s a clean way to phrase your interpretation for the GPT prompt, based on the image and your explanation:


---

Interpretation:
When tossing a fair coin, the probability of getting a tail on any single toss is 0.5. However, as we observe increasingly unlikely sequences — such as getting two tails in a row, then three tails, and eventually five tails — the probability of those events occurring purely by chance becomes much smaller. In statistical terms, the p-value decreases as the observed outcome becomes more extreme, providing stronger evidence against the null hypothesis that the coin is fair.


---

If you want, I can also make a more visual step-by-step explanation linking the sequence of tails to decreasing p-values so it matches exactly with the image’s coin graphics. Would you like me to prepare that?

One-line definition:
Log transformation is a technique that applies the natural logarithm (or another log base) to skewed data to reduce the impact of extreme values and make the distribution more symmetrical for outlier treatment.




I see from your screenshot that you’ve already applied pd.get_dummies() to your loan_df, so the categorical variables are now represented as True/False dummy columns along with your numeric columns like ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, and Credit_History.

At this stage, since the categorical features are already encoded into binary values (0/1 after converting True/False), the standard approach is:


---

Step 1 – Convert True/False to 0/1

get_dummies() gives boolean values by default if dtype=bool. To make scaling easier:

loan_df = loan_df.astype({col: int for col in loan_df.select_dtypes('bool').columns})


---

Step 2 – Scale only numeric columns that need it

Binary dummy columns (0/1) do not need scaling in most cases, because they already have a fixed, comparable range.
You only scale continuous numeric features:

from sklearn.preprocessing import StandardScaler

# Identify continuous numeric columns (exclude binary dummies)
continuous_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']

scaler = StandardScaler()
loan_df[continuous_cols] = scaler.fit_transform(loan_df[continuous_cols])

If you prefer Min-Max scaling:

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
loan_df[continuous_cols] = scaler.fit_transform(loan_df[continuous_cols])


---

Step 3 – Train-Test Split

from sklearn.model_selection import train_test_split

X = loan_df.drop('Loan_Status', axis=1)
y = loan_df['Loan_Status']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(X_train.shape, X_test.shape)


---

✅ Key takeaway:
Since your dataset now has mixed scaled numeric features and 0/1 dummies, scale only the continuous numeric features — scaling the one-hot columns usually isn’t necessary unless you’re feeding them into a neural network and want all features to be normalized.


---

If you want, I can rewrite this into a single pipeline so that imputation, encoding, scaling, and splitting happen in one go without manual column handling. That would make your workflow cleaner and reusable. Do you want me to prepare that?


Here’s a clean, notebook-friendly explanation you can paste directly under your data preprocessing section:


---

Handling Missing Values – Target Variable vs. Non-Predictive Columns

In this dataset, the target variable Loan_Status contains some missing values. Since imputing missing values in the target variable can bias the model and is generally not recommended for supervised learning, we have decided to impute with the mode here only for illustration purposes. In a real-world predictive modeling scenario, rows with missing target values would typically be excluded from training and handled separately (e.g., predicted later once the model is trained).

Similarly, the Loan_ID column is a unique identifier and does not contribute to prediction. Normally, such columns would be dropped during model training to avoid introducing noise. However, for the purpose of demonstrating data dimensionality concepts and working with sparse data in this example, we have retained Loan_ID and imputed its missing values with the mode as well.

> Note: The imputation choices made here for the target variable and Loan_ID are for demonstration only and would not be used in a production-level machine learning workflow.




---

If you want, I can also give you a shorter version so it looks cleaner in the notebook but still keeps the warning note. Would you like me to prepare that too?


Here’s a clear comment line you can add in your notebook so future readers know why Loan_ID is kept here, even though it’s not a predictive feature:

# NOTE: Loan_ID is normally dropped before modeling because it has no predictive value.
#       We have kept it here only for demonstration purposes:
#       - To show how high-cardinality features create sparse data when one-hot encoded.
#       - To illustrate the curse of dimensionality.
#       - To discuss grouping high-cardinality categories into fewer buckets 
#         (a process called binning or category reduction, also known as frequency encoding or bucketing).


---

💡 In your case, the technique you were thinking of is bucketing (binning) or category reduction.

Bucketing/Binning: Grouping many distinct categories into a smaller set of broader categories.

Example: 300 district codes → 4–5 geographic zones.


For modeling, bucketing reduces the number of dummy variables and helps combat the curse of dimensionality when dealing with high-cardinality categorical variables.

If you want, I can also give you a short code snippet to automatically bucket such high-cardinality categories into fewer groups. Would you like me to prepare that?


